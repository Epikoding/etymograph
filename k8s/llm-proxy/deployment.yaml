apiVersion: apps/v1
kind: Deployment
metadata:
  name: llm-proxy
  namespace: etymograph
  labels:
    app: llm-proxy
spec:
  replicas: 2
  selector:
    matchLabels:
      app: llm-proxy
  template:
    metadata:
      labels:
        app: llm-proxy
    spec:
      containers:
        - name: llm-proxy
          image: ghcr.io/epikoding/etymograph-llm-proxy:latest
          imagePullPolicy: IfNotPresent
          ports:
            - containerPort: 8081
          env:
            - name: PORT
              valueFrom:
                configMapKeyRef:
                  name: etymograph-config
                  key: LLM_PROXY_PORT
            - name: OLLAMA_URL
              valueFrom:
                configMapKeyRef:
                  name: etymograph-config
                  key: OLLAMA_URL
            - name: OLLAMA_MODEL
              valueFrom:
                configMapKeyRef:
                  name: etymograph-config
                  key: OLLAMA_MODEL
            - name: LLM_PROVIDER
              valueFrom:
                secretKeyRef:
                  name: etymograph-secrets
                  key: LLM_PROVIDER
            - name: GEMINI_API_KEY
              valueFrom:
                secretKeyRef:
                  name: etymograph-secrets
                  key: GEMINI_API_KEY
            - name: GEMINI_MODEL
              valueFrom:
                secretKeyRef:
                  name: etymograph-secrets
                  key: GEMINI_MODEL
          resources:
            requests:
              memory: "64Mi"
              cpu: "50m"
            limits:
              memory: "128Mi"
              cpu: "500m"
          livenessProbe:
            httpGet:
              path: /health
              port: 8081
            initialDelaySeconds: 5
            periodSeconds: 10
          readinessProbe:
            httpGet:
              path: /health
              port: 8081
            initialDelaySeconds: 3
            periodSeconds: 5
